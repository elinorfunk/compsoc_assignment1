{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 \n",
    "\n",
    "Elinor Funk, Leland Whitlock, Marlo Anzarut \n",
    "\n",
    "github: https://github.com/elinorfunk/compsoc_assignment1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\whitl\\AppData\\Local\\Temp\\ipykernel_15748\\1781237093.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://ic2s2-2023.org/program')\n",
    "bs = BeautifulSoup(page.content)\n",
    "italics = bs.find_all('i')\n",
    "name_list = []\n",
    "for i in italics:\n",
    "    names = i.get_text()\n",
    "    clean_names = names.replace(\"Chair:\", \"\")\n",
    "    name_list += clean_names.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(name_list)):\n",
    "    name_list[i] = name_list[i].strip()\n",
    "    name_list[i] = name_list[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = pd.DataFrame(name_list, columns=['names'])\n",
    "no_dupes = name_df.drop_duplicates(keep='first', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dupes.to_csv(\"data/names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read author data from week 2, exercise 2\n",
    "wd = os.getcwd()\n",
    "data = wd[:wd.rfind(\"\\\\\")] + \"\\\\data\"\n",
    "data = data.replace(\"\\\\\", \"/\")\n",
    "\n",
    "part_one = pd.read_csv(f\"{data}/part_two_info.csv\")\n",
    "part_two = pd.read_csv(f\"{data}/part_two_info_2.csv\")\n",
    "complete_list = pd.concat([part_one, part_two], ignore_index=True)\n",
    "\n",
    "final_df = complete_list.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering/Works Requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove authors with works_count < 5, > 1000\n",
    "non_contributors = final_df[(final_df.works_count < 5) | (final_df.works_count > 5000)].index\n",
    "pmos = final_df.drop(non_contributors)\n",
    "pmos = pmos.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the request url with the given authors and filters\n",
    "def build_request(author_ids, cursor=\"*\"):\n",
    "    auth = \"author.id:\"\n",
    "    for id in author_ids:\n",
    "        auth += f\"{id}|\"\n",
    "    auth = auth[:-1]\n",
    "\n",
    "\n",
    "    concept_filter = \"concept.id:https://openalex.org/C17744445|https://openalex.org/C144024400|https://openalex.org/C15744967|https://openalex.org/C162324750,concept.id:https://openalex.org/C121332964|https://openalex.org/C41008148|https://openalex.org/C33923547\"\n",
    "    url = f\"https://api.openalex.org/works?per-page=200&cursor={cursor}&filter={auth},{concept_filter},cited_by_count:>10,authors_count:<10\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the authors used in the filter given a start index (max 25)\n",
    "def get_authors(start):\n",
    "    query_list = []\n",
    "    if start + 24 > len(pmos):\n",
    "        end = len(pmos) - 1\n",
    "    else:\n",
    "        end = start + 24\n",
    "    for i in pmos.loc[start:end][\"id\"]:\n",
    "        id = i[i.find('A'):]\n",
    "        query_list.append(id)\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the json of a work item in the html response\n",
    "def get_work_row(item):\n",
    "    id = item['id']\n",
    "    pub_year = item['publication_year']\n",
    "    cite_count = item['cited_by_count']\n",
    "    authors = [item['authorships'][a]['author'].get('id') for a in range(len(item['authorships']))]\n",
    "    title = item['title']\n",
    "    aii = item['abstract_inverted_index']\n",
    "    \n",
    "    return [id, pub_year, cite_count, authors, title, aii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor\n",
    "works_df = pd.DataFrame(columns=['id', 'publication_year', 'cited_by_count', 'author_ids', 'title', 'abstract_inverted_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the next batch of results using the batch # (counter)\n",
    "def fetch_results(counter):\n",
    "    next_cursor = '*'\n",
    "\n",
    "    while (next_cursor):\n",
    "        # form request url\n",
    "        r = build_request(get_authors(counter * 25), next_cursor)\n",
    "        response = requests.get(r)\n",
    "        if not response:\n",
    "            break\n",
    "        rjson = response.json()\n",
    "\n",
    "        # set next_cursor for paging\n",
    "        next_cursor = rjson['meta'].get('next_cursor')\n",
    "        \n",
    "        # add results of response to works_df\n",
    "        for work in rjson['results']:\n",
    "            works_df.loc[len(works_df.index)] = get_work_row(work)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the works dataset by fetching all authors\n",
    "for i in range(45):\n",
    "    fetch_results(i)\n",
    "\n",
    "works_df.drop_duplicates(subset='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
